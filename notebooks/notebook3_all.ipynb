{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "split-cleaner",
   "metadata": {},
   "source": [
    "# Evaluate ALL features\n",
    "\n",
    "Similar to notebook 3 we package everything inside a for loop to evaluate all the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1201329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO REMOVE when notebook is stable\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-logic",
   "metadata": {},
   "source": [
    "### Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "living-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import seaborn\n",
    "import tarfile\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from anndata import read_h5ad\n",
    "\n",
    "# tissue_purifier import\n",
    "import tissue_purifier as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e0252",
   "metadata": {},
   "source": [
    "### Download the annotated anndata object \n",
    "\n",
    "Altenatively you can use the anndata files generated by running notebook2_all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unique-nashville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anndata_sick3.h5ad', 'anndata_sick1.h5ad', 'anndata_sick2.h5ad', 'anndata_wt2.h5ad', 'anndata_wt1.h5ad', 'anndata_wt3.h5ad']\n"
     ]
    }
   ],
   "source": [
    "import tissue_purifier.io\n",
    "\n",
    "bucket_name = \"ld-data-bucket\"\n",
    "annotated_anndata_source_path = \"tissue-purifier/annotated_slideseq_testis_anndata_h5ad.tar.gz\"\n",
    "annotated_anndata_dest_path = \"./annotated_slideseq_testis_anndata_h5ad.tar.gz\"\n",
    "annotated_anndata_dest_folder = \"./testis_anndata_annotated\"\n",
    "\n",
    "#tp.io.download_from_bucket(bucket_name, annotated_anndata_source_path, annotated_anndata_dest_path)   \n",
    "#with tarfile.open(annotated_anndata_dest_path, \"r:gz\") as fp:\n",
    "#    fp.extractall(path=annotated_anndata_dest_folder)\n",
    "    \n",
    "# Make a list of all the h5ad files in the annotated_anndata_dest_folder\n",
    "fname_list = []\n",
    "for f in os.listdir(annotated_anndata_dest_folder):\n",
    "    if f.endswith('.h5ad'):\n",
    "        fname_list.append(f)\n",
    "print(fname_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91db1d68",
   "metadata": {},
   "source": [
    "### Decide how to filter the anndata object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c2886b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter cells parameters\n",
    "fc_bc_min_umi = 200                  # filter cells with too few UMI\n",
    "fc_bc_max_umi = 3000                 # filter cells with too many UMI\n",
    "fc_bc_min_n_genes_by_counts = 10     # filter cells with too few GENES\n",
    "fc_bc_max_n_genes_by_counts = 2500   # filter cells with too many GENES\n",
    "fc_bc_max_pct_counts_mt = 5          # filter cells with mitocrondial fraction too high\n",
    "\n",
    "# filter genes parameters\n",
    "fg_bc_min_cells_by_counts = 3000      # filter genes which appear in too few CELLS\n",
    "\n",
    "# filter rare cell types parameters\n",
    "fctype_bc_min_cells_absolute = 100   # filter cell-types which are too RARE in absolute number\n",
    "fctype_bc_min_cells_frequency = 0.01 # filter cell-types which are too RARE in relative abundance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff84026",
   "metadata": {},
   "source": [
    "### Open the first annotated anndata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a4538e-38ae-444a-9a38-aa0170169354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 33441 × 23514\n",
       "    obs: 'x', 'y', 'cell_type'\n",
       "    obsm: 'barlow', 'dino', 'ncv_k10', 'ncv_k100', 'ncv_k20', 'ncv_k200', 'ncv_k50', 'ncv_k500', 'simclr', 'vae'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = read_h5ad(filename=os.path.join(annotated_anndata_dest_folder, fname_list[0]))\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852f24c-9269-4208-a9bf-69d85507896b",
   "metadata": {},
   "source": [
    "### compute few metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f62b476e-ff96-4a34-801f-26593d67146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             counts     freqs\n",
      "categories                   \n",
      "ES            12552  0.375348\n",
      "Endothelial     417  0.012470\n",
      "Leydig          340  0.010167\n",
      "Macrophage      623  0.018630\n",
      "Myoid           969  0.028976\n",
      "RS             6780  0.202745\n",
      "SPC            8069  0.241291\n",
      "SPG            2238  0.066924\n",
      "Sertoli        1453  0.043450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 33441 × 23514\n",
       "    obs: 'x', 'y', 'cell_type', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'keep_ctype'\n",
       "    var: 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'\n",
       "    obsm: 'barlow', 'dino', 'ncv_k10', 'ncv_k100', 'ncv_k20', 'ncv_k200', 'ncv_k50', 'ncv_k500', 'simclr', 'vae'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "cell_type_key = \"cell_type\"\n",
    "\n",
    "# mitocondria metrics\n",
    "adata.var['mt'] = adata.var_names.str.startswith('mt-')  # annotate the group of mitochondrial genes as 'mt'\n",
    "sc.pp.calculate_qc_metrics(adata, qc_vars=['mt'], percent_top=None, log1p=False, inplace=True)\n",
    "\n",
    "# counts cells frequency\n",
    "tmp = adata.obs[cell_type_key].values.describe()\n",
    "print(tmp)\n",
    "mask1 = (tmp[\"counts\"] > fctype_bc_min_cells_absolute)\n",
    "mask2 = (tmp[\"freqs\"] > fctype_bc_min_cells_frequency)\n",
    "mask = mask1 * mask2\n",
    "cell_type_keep = set(tmp[mask].index.values)\n",
    "adata.obs[\"keep_ctype\"] = adata.obs[\"cell_type\"].apply(lambda x: x in cell_type_keep)\n",
    "\n",
    "# Note that adata has extra annotation now\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f13f2",
   "metadata": {},
   "source": [
    "### Filter out cells, genes and cell-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ec3d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata[adata.obs[\"total_counts\"] > fc_bc_min_umi, :] \n",
    "adata = adata[adata.obs[\"total_counts\"] < fc_bc_max_umi, :] \n",
    "adata = adata[adata.obs[\"n_genes_by_counts\"] > fc_bc_min_n_genes_by_counts, :] \n",
    "adata = adata[adata.obs[\"n_genes_by_counts\"] < fc_bc_max_n_genes_by_counts, :] \n",
    "adata = adata[adata.obs[\"pct_counts_mt\"] < fc_bc_max_pct_counts_mt, :]\n",
    "adata = adata[adata.obs[\"keep_ctype\"] == True, :]\n",
    "adata = adata[:, adata.var[\"n_cells_by_counts\"] > fg_bc_min_cells_by_counts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53534ab-1842-435e-808d-254e48d59be0",
   "metadata": {},
   "source": [
    "# Loop to train multiple gene_regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff962c9-870d-463d-a732-8d2769be2698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barlow\n",
      "[iter 1]  loss: 20566677786.0000\n",
      "Training completed in 0.4303302764892578 seconds\n",
      "saved file gr_barlow_no_regularization.pt\n",
      "[iter 1]  loss: 21545082116.0000\n",
      "Training completed in 0.42090439796447754 seconds\n",
      "saved file gr_barlow_l1_0.01.pt\n",
      "[iter 1]  loss: 21181772996.0000\n",
      "Training completed in 0.41753530502319336 seconds\n",
      "saved file gr_barlow_l1_0.1.pt\n",
      "[iter 1]  loss: 22615514253.0000\n",
      "Training completed in 0.41670942306518555 seconds\n",
      "saved file gr_barlow_l1_1.0.pt\n",
      "[iter 1]  loss: 21839123472.0000\n",
      "Training completed in 0.44511938095092773 seconds\n",
      "saved file gr_barlow_l2_0.01.pt\n",
      "[iter 1]  loss: 23766348756.0000\n",
      "Training completed in 0.42088770866394043 seconds\n",
      "saved file gr_barlow_l2_0.1.pt\n",
      "[iter 1]  loss: 22213972888.0000\n",
      "Training completed in 0.4192237854003906 seconds\n",
      "saved file gr_barlow_l2_1.0.pt\n",
      "dino\n",
      "[iter 1]  loss: 22176786714.0000\n",
      "Training completed in 0.4129457473754883 seconds\n",
      "saved file gr_dino_no_regularization.pt\n",
      "[iter 1]  loss: 24574528772.0000\n",
      "Training completed in 0.42188405990600586 seconds\n",
      "saved file gr_dino_l1_0.01.pt\n",
      "[iter 1]  loss: 24258245828.0000\n",
      "Training completed in 0.4234733581542969 seconds\n",
      "saved file gr_dino_l1_0.1.pt\n",
      "[iter 1]  loss: 22182245517.0000\n",
      "Training completed in 0.43961143493652344 seconds\n",
      "saved file gr_dino_l1_1.0.pt\n",
      "[iter 1]  loss: 21066368016.0000\n",
      "Training completed in 0.4245777130126953 seconds\n",
      "saved file gr_dino_l2_0.01.pt\n",
      "[iter 1]  loss: 23350866900.0000\n",
      "Training completed in 0.4250493049621582 seconds\n",
      "saved file gr_dino_l2_0.1.pt\n",
      "[iter 1]  loss: 20531090328.0000\n",
      "Training completed in 0.421680212020874 seconds\n",
      "saved file gr_dino_l2_1.0.pt\n",
      "simclr\n",
      "[iter 1]  loss: 21328541978.0000\n",
      "Training completed in 0.4130289554595947 seconds\n",
      "saved file gr_simclr_no_regularization.pt\n",
      "[iter 1]  loss: 21504959748.0000\n",
      "Training completed in 0.43024754524230957 seconds\n",
      "saved file gr_simclr_l1_0.01.pt\n",
      "[iter 1]  loss: 22735660228.0000\n",
      "Training completed in 0.43538689613342285 seconds\n",
      "saved file gr_simclr_l1_0.1.pt\n",
      "[iter 1]  loss: 23425256589.0000\n",
      "Training completed in 0.4234437942504883 seconds\n",
      "saved file gr_simclr_l1_1.0.pt\n",
      "[iter 1]  loss: 24012842000.0000\n",
      "Training completed in 0.42484426498413086 seconds\n",
      "saved file gr_simclr_l2_0.01.pt\n",
      "[iter 1]  loss: 20445041620.0000\n",
      "Training completed in 0.42274975776672363 seconds\n",
      "saved file gr_simclr_l2_0.1.pt\n",
      "[iter 1]  loss: 19823467416.0000\n",
      "Training completed in 0.4354221820831299 seconds\n",
      "saved file gr_simclr_l2_1.0.pt\n",
      "barlow_pca\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2487/944171180.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mpreprocess_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'z_score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mapply_pca\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             n_components=9,)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disks/persistent_disk/REPOS/tissue_purifier/src/tissue_purifier/genex/gene_utils.py\u001b[0m in \u001b[0;36mmake_gene_dataset_from_anndata\u001b[0;34m(anndata, cell_type_key, covariate_key, preprocess_strategy, apply_pca, n_components)\u001b[0m\n\u001b[1;32m    122\u001b[0m     return GeneDataset(\n\u001b[1;32m    123\u001b[0m         \u001b[0mcell_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcell_type_ids_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mcovariates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_covariate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mcounts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcounts_ng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mk_cell_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk_cell_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "from tissue_purifier.genex import *\n",
    "\n",
    "covariate_keys = ['barlow', 'dino', 'simclr', 'barlow_pca', 'dino_pca', 'simclr_pca', \n",
    "                  'ncv_k100', 'ncv_k20', 'ncv_k200', 'ncv_k50', 'ncv_k500'] \n",
    "n_train_steps = 10\n",
    "\n",
    "l1_strengths = [0.01, 0.1, 1.0]\n",
    "l2_strengths = [0.01, 0.1, 1.0]\n",
    "\n",
    "gr = GeneRegression()\n",
    "gr.configure_optimizer(optimizer_type='adam', lr=5E-3)\n",
    "\n",
    "for covariate_key in covariate_keys:\n",
    "    print(covariate_key)\n",
    "\n",
    "    # decide the subsample size (to fit into GPU memory)\n",
    "    if covariate_key.startswith(\"ncv\") or covariate_key.endswith(\"_pca\"):\n",
    "        subsample_size_cells = 2000\n",
    "    else:\n",
    "        subsample_size_cells = 200 \n",
    "    \n",
    "    \n",
    "    if covariate_key.endswith(\"_pca\"):\n",
    "        \n",
    "        # make gene dataset with PCA\n",
    "        gene_dataset = make_gene_dataset_from_anndata(\n",
    "            anndata=adata,\n",
    "            cell_type_key='cell_type',\n",
    "            covariate_key=covariate_key.split(\"_\")[0],\n",
    "            preprocess_strategy='z_score',\n",
    "            apply_pca=True,\n",
    "            n_components=9,)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # make the dataset without PCA\n",
    "        gene_dataset = make_gene_dataset_from_anndata(\n",
    "            anndata=adata,\n",
    "            cell_type_key='cell_type',\n",
    "            covariate_key=covariate_key,\n",
    "            preprocess_strategy='raw',\n",
    "            apply_pca=False)\n",
    "    \n",
    "    # split into train/test/val (note that we provide the random_state for reproducibility) \n",
    "    train_test_val_dataset = next(iter(train_test_val_split(gene_dataset, random_state=0)))\n",
    "    torch.save(train_test_val_dataset, \"gr_{}_dataset.pt\".format(covariate_key))\n",
    "    train_dataset, test_dataset, val_dataset = train_test_val_dataset\n",
    "    \n",
    "    # train with no regularization\n",
    "    gr.train(\n",
    "        dataset=train_dataset,\n",
    "        n_steps=n_train_steps,\n",
    "        print_frequency=10000,\n",
    "        use_covariates=True,\n",
    "        l1_regularization_strength=None,\n",
    "        l2_regularization_strength=None,\n",
    "        eps_range=(1.0E-5, 1.0E-2),\n",
    "        subsample_size_cells=subsample_size_cells,\n",
    "        subsample_size_genes=None,\n",
    "        from_scratch=True)\n",
    "    gr.save_ckpt(\"gr_{}_no_regularization.pt\".format(covariate_key))\n",
    "    print(\"saved file gr_{}_no_regularization.pt\".format(covariate_key))\n",
    "    \n",
    "    # train with l1 regularization\n",
    "    for l1 in l1_strengths: \n",
    "        gr.train(\n",
    "            dataset=train_dataset,\n",
    "            n_steps=n_train_steps,\n",
    "            print_frequency=10000,\n",
    "            use_covariates=True,\n",
    "            l1_regularization_strength=l1,\n",
    "            l2_regularization_strength=None,\n",
    "            eps_range=(1.0E-5, 1.0E-2),\n",
    "            subsample_size_cells=subsample_size_cells,\n",
    "            subsample_size_genes=None,\n",
    "            from_scratch=True)\n",
    "        gr.save_ckpt(\"gr_{}_l1_{}.pt\".format(covariate_key, l1))\n",
    "        print(\"saved file gr_{}_l1_{}.pt\".format(covariate_key, l1))\n",
    "    \n",
    "    # train with l2 regularization\n",
    "    for l2 in l2_strengths: \n",
    "        gr.train(\n",
    "            dataset=train_dataset,\n",
    "            n_steps=n_train_steps,\n",
    "            print_frequency=10000,\n",
    "            use_covariates=True,\n",
    "            l1_regularization_strength=None,\n",
    "            l2_regularization_strength=l2,\n",
    "            eps_range=(1.0E-5, 1.0E-2),\n",
    "            subsample_size_cells=subsample_size_cells,\n",
    "            subsample_size_genes=None,\n",
    "            from_scratch=True)\n",
    "        gr.save_ckpt(\"gr_{}_l2_{}.pt\".format(covariate_key, l2))\n",
    "        print(\"saved file gr_{}_l2_{}.pt\".format(covariate_key, l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0688a21-77db-44af-b2af-7042a1d2d6d6",
   "metadata": {},
   "source": [
    "# Compare all regressions\n",
    "\n",
    "1. check the loss function converged\n",
    "2. for each covariate_key compute the ration Q_with_covariance vs Q_no_covariance to select the best regularization\n",
    "3. compare across covariate_key\n",
    "\n",
    "REMOVE q_empirical from predict b/c it is misleading\n",
    "I don't like the fact that epsilon is clustered by cell_type. Is it because the model is un-identifiable (both beta0 and eps are K x G). Think about changing model to have eps only being gene dependent. (as by Dylan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11553783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu113.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu113:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
