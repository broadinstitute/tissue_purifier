{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "korean-scanning",
   "metadata": {},
   "source": [
    "# Test tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "technical-baghdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "happy-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import lightly\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from tissue_purifier.util_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-house",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vertical-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/jupyter/data/slide-seq/original_data/\"\n",
    "root = \"/Users/ldalessi/REPOS/ML_for_slideseq/data/\"\n",
    "\n",
    "df_wt1 = pd.read_csv(root + \"wt1.csv\")\n",
    "df_wt2 = pd.read_csv(root + \"wt2.csv\")\n",
    "df_wt3 = pd.read_csv(root + \"wt3.csv\")\n",
    "df_dis1 = pd.read_csv(root + \"dis1.csv\")\n",
    "df_dis2 = pd.read_csv(root + \"dis2.csv\")\n",
    "df_dis3 = pd.read_csv(root + \"dis3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-testimony",
   "metadata": {},
   "source": [
    "### Split into left and right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sunset-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wt1_left = df_wt1[df_wt1[\"x\"] < df_wt1[\"x\"].median()]\n",
    "df_wt1_right = df_wt1[df_wt1[\"x\"] >= df_wt1[\"x\"].median()]\n",
    "\n",
    "df_wt2_left = df_wt2[df_wt2[\"x\"] < df_wt2[\"x\"].median()]\n",
    "df_wt2_right = df_wt2[df_wt2[\"x\"] >= df_wt2[\"x\"].median()]\n",
    "\n",
    "df_wt3_left = df_wt3[df_wt3[\"x\"] < df_wt3[\"x\"].median()]\n",
    "df_wt3_right = df_wt3[df_wt3[\"x\"] >= df_wt3[\"x\"].median()]\n",
    "\n",
    "df_dis1_left = df_dis1[df_dis1[\"x\"] < df_dis1[\"x\"].median()]\n",
    "df_dis1_right = df_dis1[df_dis1[\"x\"] >= df_dis1[\"x\"].median()]\n",
    "\n",
    "df_dis2_left = df_dis2[df_dis2[\"x\"] < df_dis2[\"x\"].median()]\n",
    "df_dis2_right = df_dis2[df_dis2[\"x\"] >= df_dis2[\"x\"].median()]\n",
    "\n",
    "df_dis3_left = df_dis3[df_dis3[\"x\"] < df_dis3[\"x\"].median()]\n",
    "df_dis3_right = df_dis3[df_dis3[\"x\"] >= df_dis3[\"x\"].median()]\n",
    "\n",
    "labels_left = [\"wt\", \"wt\", \"wt\", \"dis\", \"dis\", \"dis\"]\n",
    "labels_right = [\"wt\", \"wt\", \"wt\", \"dis\", \"dis\", \"dis\"]\n",
    "filename_left = [\"wt1\", \"wt2\", \"wt3\", \"dis1\", \"dis2\", \"dis3\"]\n",
    "filename_right = [\"wt1\", \"wt2\", \"wt3\", \"dis1\", \"dis2\", \"dis3\"]                \n",
    "all_df_left = [df_wt1_left, df_wt2_left, df_wt3_left, df_dis1_left, df_dis2_left, df_dis3_left]\n",
    "all_df_right = [df_wt1_left, df_wt2_left, df_wt3_left, df_dis1_left, df_dis2_left, df_dis3_left]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-marble",
   "metadata": {},
   "source": [
    "### Define the defaults values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "substantial-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "batch_size = 128\n",
    "seed = 1\n",
    "max_epochs = 100\n",
    "input_size = 224\n",
    "num_ftrs = 32\n",
    "pixel_size = 4.0\n",
    "crop_size = input_size\n",
    "input_channels = 9\n",
    "n_element_min = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-favorite",
   "metadata": {},
   "source": [
    "### TrainTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acute-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = torchvision.transforms.Compose([\n",
    "    DropoutSparseTensor(dropout=(0.0, 0.4)),\n",
    "    StackTensor(dim=-4),\n",
    "    RandomGaussianBlur(sigma=(1.0, 1.0)),\n",
    "    torchvision.transforms.RandomAffine(degrees=180, \n",
    "                                        scale=(0.75, 1.25), \n",
    "                                        shear=0.0, \n",
    "                                        interpolation=torchvision.transforms.InterpolationMode.NEAREST, \n",
    "                                        fill=0),\n",
    "    RandomIntensity(factor=(0.7, 1.3)),\n",
    "    torchvision.transforms.CenterCrop(size=crop_size),\n",
    "    torchvision.transforms.Resize(input_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-henry",
   "metadata": {},
   "source": [
    "### Left TrainDataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "comfortable-bible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements ---> 15829\n",
      "The dense shape of the image is -> torch.Size([9, 611, 1164])\n",
      "number of elements ---> 16529\n",
      "The dense shape of the image is -> torch.Size([9, 602, 845])\n",
      "number of elements ---> 19603\n",
      "The dense shape of the image is -> torch.Size([9, 602, 1170])\n",
      "number of elements ---> 13597\n",
      "The dense shape of the image is -> torch.Size([9, 601, 1170])\n",
      "number of elements ---> 21387\n",
      "The dense shape of the image is -> torch.Size([9, 596, 1170])\n",
      "number of elements ---> 16718\n",
      "The dense shape of the image is -> torch.Size([9, 523, 1150])\n"
     ]
    }
   ],
   "source": [
    "sparse_images_left = [\n",
    "    SparseImage.from_panda(\n",
    "    df, x=\"x\", y=\"y\", category=\"max_cell_type\", \n",
    "    pixel_size=4.0, padding=10) for df in all_df_left\n",
    "]\n",
    "\n",
    "#n_crops_for_tissue = int(numpy.ceil(float(batch_size) / len(sparse_images_left)))\n",
    "n_crops_for_tissue = 2\n",
    "\n",
    "dataset_train_left = SparseDataset(x=sparse_images_left, \n",
    "                                   y=labels_left,\n",
    "                                   z=filename_left,\n",
    "                                   transform_x=RandomCropSparseTensor(n_crops=n_crops_for_tissue, \n",
    "                                                                      crop_size=int(1.5*crop_size),\n",
    "                                                                      n_element_min=100),\n",
    "                                   transform_y=Interleave(n_repeat=n_crops_for_tissue)) \n",
    "\n",
    "dataloader_train_left = DataLoaderWithLoad(\n",
    "    dataset_train_left,\n",
    "    batch_size=dataset_train_left.__len__(),\n",
    "    collate_fn=SpecialCollateFn(transform=train_transform, simclr_output=False),\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "negative-instruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1] ['wt1', 'wt1']\n",
      "[1, 1] ['wt2', 'wt2']\n",
      "[1, 1] ['wt3', 'wt3']\n",
      "[0, 0] ['dis1', 'dis1']\n",
      "[0, 0] ['dis2', 'dis2']\n",
      "[0, 0] ['dis3', 'dis3']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dis': 0, 'wt': 1}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(dataset_train_left.__len__()):\n",
    "    a,b,c = dataset_train_left.__getitem__(i)\n",
    "    print(b,c)\n",
    "dataset_train_left.labels_to_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "posted-effects",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 9, 224, 224]) torch.Size([12]) 12\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "imgs, labels, fname = next(iter(dataloader_train_left))\n",
    "print(imgs.shape, labels.shape, len(fname))\n",
    "show_tensor(imgs[:10, 0], figsize=(12,4), n_col=5, cmap='hot', normalize_range=(0.0, 1.0))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "choice-firewall",
   "metadata": {},
   "source": [
    "### Take the pretrained backbone and add a linear classifier on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "joint-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Classifier(torch.nn.Module):\n",
    "#    def __init__(self, backbone: torch.nn.Module, n_classes: int):\n",
    "#        super().__init__()\n",
    "#        self.n_classes = n_classes\n",
    "#        self.backbone = backbone\n",
    "#        self.prediction_head = torch.nn.Linear(in_features=128, out_features=n_classes, bias=True)\n",
    "#        \n",
    "#    def forward(self, x: torch.Tensor):\n",
    "#        with torch.no_grad():\n",
    "#            y = self.backbone(x)\n",
    "#        return self.prediction_head(y)\n",
    "#    \n",
    "## The backbone is the one trained with Contrastive Learning (here I am using resnet18)\n",
    "#resnet18 = torch.hub.load('pytorch/vision:v0.9.0', 'resnet18', pretrained=True)\n",
    "#resnet18.conv1 = torch.nn.Conv2d(9, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "#resnet18.fc = torch.nn.Linear(in_features=512, out_features=128, bias=True)\n",
    "\n",
    "\n",
    "class RandomClassifier(torch.nn.Module):\n",
    "    def __init__(self, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.prediction_head = torch.nn.Linear(in_features=12, out_features=n_classes, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            batch_size = x.shape[0]\n",
    "            y = torch.randn((batch_size, 12), dtype=x.dtype, device=x.device)\n",
    "        return self.prediction_head(y)\n",
    "\n",
    "dummy_classifier = RandomClassifier(n_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-afternoon",
   "metadata": {},
   "source": [
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tissue_purifier.loss import NoisyLoss\n",
    "\n",
    "classifier = Classifier(backbone=resnet18, n_classes=2)\n",
    "optimizer = optimizer = torch.optim.Adam(classifier.parameters(), lr=1E-3, betas=(0.9, 0.999))\n",
    "criterion = NoisyLoss()\n",
    "\n",
    "max_epoch = 2\n",
    "for epoch in range(max_epoch):\n",
    "    for counter, (imgs, labels, fnames) in enumerate(dataloader_train_left):\n",
    "        #print(imgs.shape, labels.shape, fnames)\n",
    "        \n",
    "        outputs = classifier(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-enterprise",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
