{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indian-institute",
   "metadata": {},
   "source": [
    "# Contrastive Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-creativity",
   "metadata": {},
   "source": [
    "### Add the src folder to the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hybrid-landing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root_path = os.path.dirname(os.getcwd())\n",
    "src_path = os.path.join(root_path, \"src\")\n",
    "sys.path.insert(0, src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "enclosed-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "broken-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import tissue_purifier as tp\n",
    "from tissue_purifier.data_utils.helpers import define_train_loader, define_test_loader\n",
    "from tissue_purifier.model_utils.simclr import SimCLR\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "from tissue_purifier.data_utils import SpatialAutocorrelation\n",
    "from tissue_purifier.plot_utils import plot_all_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pursuant-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tissue_purifier.model_utils.helpers import define_simclr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-crystal",
   "metadata": {},
   "source": [
    "### Read in the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unlimited-thompson",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEPTUNE_API_TOKEN=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZTkyYmJiYi0wN2E1LTRkY2YtOWU3Ny1kNjhjYmM3ZTVkNWEifQ==\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "tough-parliament",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(dd, separator='_', prefix=''):\n",
    "    return {prefix + separator + k if prefix else k: v\n",
    "            for kk, vv in dd.items()\n",
    "            for k, v in flatten_dict(vv, separator, kk).items()\n",
    "            } if isinstance(dd, dict) else {prefix: dd}\n",
    "\n",
    "with open(\"./config.yaml\", 'r') as stream:\n",
    "    config = yaml.safe_load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "figured-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "simclr = define_simclr(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "greek-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "transf1 = simclr.predict_transform[\"sampler\"]\n",
    "transf2 = simclr.predict_transform[\"reference\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "super-editing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    DropoutSparseTensor(dropout=(0.0,0.4), same_dropout_rate_for_all_elements=False)\n",
       "    SparseToDense\n",
       "    RandomGaussianBlur(sigma=(1.0,4.0))\n",
       "    RandomAffine(degrees=[-180.0, 180.0], translate=(0.05, 0.05), scale=(0.75, 1.25), shear=[-0.0, 0.0])\n",
       "    CenterCrop(size=(224, 224))\n",
       "    RandomStraightCut(p=0.5, occlusion_fraction=0.05)\n",
       "    ScaleIntensity(in_range=image, out_range=(0.0, 1.0))\n",
       "    RandomIntensity(factor=(0.7,1.3)\n",
       "    RandomVerticalFlip(p=0.5)\n",
       "    RandomHorizontalFlip(p=0.5)\n",
       "    Resize(size=224, interpolation=bilinear)\n",
       "    StackTensor(dim=-4)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "blank-intention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    SparseToDense\n",
       "    StackTensor(dim=-4)\n",
       "    RandomGaussianBlur(sigma=(1.0,4.0))\n",
       "    CenterCrop(size=(224, 224))\n",
       "    ScaleIntensity(in_range=image, out_range=(0.0, 1.0))\n",
       "    Resize(size=224, interpolation=bilinear)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(simclr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_config = flatten_dict(config, separator=\"_\", prefix='config')\n",
    "\n",
    "neptune_logger = NeptuneLogger(\n",
    "    offline_mode=True,\n",
    "    api_key=NEPTUNE_API_TOKEN,\n",
    "    project_name='cellarium/tissue-purifier', # change this to your project\n",
    "    experiment_name=\"Contrastive Learning\",\n",
    "    tags=[config[\"model_settings\"][\"BACKBONE_TYPE\"]],\n",
    "    upload_source_files=[\"config.yaml\", \"main*.py\", \"tissue_purifier/data_utils/helpers.py\"]\n",
    ")\n",
    "\n",
    "for k,v in flatten_config.items():\n",
    "    neptune_logger.experiment.log_text(k,str(v))\n",
    "\n",
    "print(yaml.dump(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-vegetarian",
   "metadata": {},
   "source": [
    "## Manual seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config['simulation']['RANDOM_SEED'])\n",
    "np.random.seed(config['simulation']['RANDOM_SEED'])\n",
    "pl.seed_everything(config['simulation']['RANDOM_SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-sharing",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"../slide-seq-data\"\n",
    "model_folder = \"../trained_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wt1 = pd.read_csv(os.path.join(data_folder, \"wt_1.csv\"))\n",
    "df_wt2 = pd.read_csv(os.path.join(data_folder, \"wt_2.csv\"))\n",
    "df_wt3 = pd.read_csv(os.path.join(data_folder, \"wt_3.csv\"))\n",
    "df_dis1 = pd.read_csv(os.path.join(data_folder, \"sick_1.csv\"))\n",
    "df_dis2 = pd.read_csv(os.path.join(data_folder, \"sick_2.csv\"))\n",
    "df_dis3 = pd.read_csv(os.path.join(data_folder, \"sick_3.csv\"))\n",
    "\n",
    "all_df = [df_wt1, df_wt2, df_wt3, df_dis1, df_dis2, df_dis3]\n",
    "all_labels_sparse_images = [0, 0, 0, 1, 1, 1]\n",
    "all_names_sparse_images = [\"wt1\", \"wt2\", \"wt3\", \"dis1\", \"dis2\", \"dis3\"]\n",
    "all_sparse_images = [tp.data_utils.SparseImage.from_panda(\n",
    "      df, \"x\", \"y\", category=\"cell_type\", pixel_size=config[\"simulation\"][\"PIXEL_SIZE\"], padding=10)\n",
    "                   for df in all_df]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-nylon",
   "metadata": {},
   "source": [
    "## Initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tp.model_utils.helpers.define_model(\n",
    "    backbone_type=config[\"model_settings\"][\"BACKBONE_TYPE\"],\n",
    "    number_of_channels=config[\"model_settings\"][\"INPUT_CHANNELS\"],\n",
    "    num_of_filters=config[\"model_settings\"][\"BACKBONE_NUM_FTRS\"],\n",
    "    projection_out_dim=config[\"model_settings\"][\"PROJECTION_OUT_DIM\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-apple",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_type = config[\"simulation\"][\"TYPE\"]\n",
    "print(\"simulation_type ->\", simulation_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-arlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "if simulation_type == 'scratch' or simulation_type == 'resume':\n",
    "    optimizer, scheduler = tp.model_utils.helpers.define_optimizer_and_scheduler(\n",
    "        model=model,\n",
    "        num_epochs=config[\"simulation\"][\"MAX_EPOCHS\"],\n",
    "        optimizer_type=config[\"optimizer\"][\"OPTIMIZER_TYPE\"],\n",
    "        learning_rate=config[\"optimizer\"][\"LEARNING_RATE\"],\n",
    "        is_scheduled=config[\"optimizer\"][\"IS_SCHEDULED\"],\n",
    "        scheduler_step_size=config[\"optimizer\"][\"SCHEDULER_STEP_SIZE\"],\n",
    "        scheduler_gamma=config[\"optimizer\"][\"SCHEDULER_GAMMA\"]\n",
    "    )\n",
    "\n",
    "    print(\"Define the trainloader\")\n",
    "    trainloader_train, _ = define_trainloaders(\n",
    "        all_sparse_images,\n",
    "        all_labels_sparse_images,\n",
    "        all_names_sparse_images,\n",
    "        config,\n",
    "        use_train_test_transform=(True, False),\n",
    "        paired_output = True\n",
    "    )\n",
    "\n",
    "    criterion = lightly.loss.NTXentLoss()\n",
    "    encoder = TrainableEncoder(model, criterion, optimizer, trainloader_train, scheduler)\n",
    "\n",
    "    if simulation_type == 'resume':\n",
    "        ckpt = torch.load(\"ckpt.pt\")\n",
    "        encoder.model.load_state_dict(ckpt)\n",
    "\n",
    "    print(\"start training\")\n",
    "    encoder.train_embedding(\n",
    "        gpus=1 if torch.cuda.is_available() else 0,\n",
    "        progress_bar_refresh_rate=0,\n",
    "        max_epochs=config[\"simulation\"][\"MAX_EPOCHS\"],\n",
    "        log_every_n_steps=10,\n",
    "        logger=neptune_logger\n",
    "    )\n",
    "    print(\"end training\")\n",
    "    os.makedirs(model_folder, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(model_folder, \"simclr_model.pt\"))\n",
    "    neptune_logger.log_artifact(os.path.join(model_folder, \"simclr_model.pt\"))\n",
    "    print(\"model written to file\")\n",
    "\n",
    "elif simulation_type == 'fully_trained':\n",
    "    encoder = TrainedEncoder(model)\n",
    "    ckpt = torch.load(\"ckpt.pt\")\n",
    "    encoder.model.load_state_dict(ckpt)\n",
    "    print(\"loaded model from file\")\n",
    "else:\n",
    "    raise Exception(\"simulation_type not recognized. Received {0}\".format(simulation_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-article",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-heritage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluation begings!\")\n",
    "print(\"Define the loaders!\")\n",
    "loader_train, loader_test = define_testloaders(all_sparse_images,\n",
    "                                               all_labels_sparse_images,\n",
    "                                               all_names_sparse_images,\n",
    "                                               config,\n",
    "                                               use_train_test_transform=(True,True))\n",
    "\n",
    "sparse_tensor_callback = SpatialAutocorrelation(modality='moran',\n",
    "                                                n_neighbours=6,\n",
    "                                                radius=None,\n",
    "                                                neigh_correct=False)\n",
    "\n",
    "#sparse_tensor_callback = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-oasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"computing the embeddings\")\n",
    "output_embed_test: EmbedOutput = encoder.embed(loader_test, sparse_tensor_callback)\n",
    "output_embed_train: EmbedOutput = encoder.embed(loader_train,  sparse_tensor_callback)\n",
    "\n",
    "torch.save(output_embed_test, \"./output_embed_test.pt\")\n",
    "torch.save(output_embed_train, \"./output_embed_train.pt\")\n",
    "\n",
    "neptune_logger.log_artifact(\"./output_embed_test.pt\")\n",
    "neptune_logger.log_artifact(\"./output_embed_train.pt\")\n",
    "print(\"embeddings logged in neptune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-dodge",
   "metadata": {},
   "source": [
    "## Load the mebedding if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_folder = \"./embeddings\"\n",
    "#\n",
    "#output_bbone_test = torch.load(os.path.join(embedding_folder, \"output_bbone_test.pt\"), map_location=torch.device('cpu'))\n",
    "#output_bbone_train = torch.load(os.path.join(embedding_folder, \"output_bbone_train.pt\"), map_location=torch.device('cpu'))\n",
    "#output_head_test = torch.load(os.path.join(embedding_folder, \"output_head_test.pt\"), map_location=torch.device('cpu'))\n",
    "#output_head_train = torch.load(os.path.join(embedding_folder, \"output_head_train.pt\"), map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-consumption",
   "metadata": {},
   "source": [
    "# knn graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"compute the knn only for head with cosine\")\n",
    "# random_samples = torch.randperm(len(loader_test.dataset.x))[:10]\n",
    "random_samples = [230, 217, 256, 196, 192, 272, 240, 109, 161, 265]\n",
    "print(\"random_samples\", random_samples)\n",
    "\n",
    "knn_test = tp.plot_utils.plot_knn_examples(output_head_test.embeddings,\n",
    "                                           loader_test,\n",
    "                                           n_neighbors=5,\n",
    "                                           examples=random_samples,\n",
    "                                           cmap=plt.cm.viridis,\n",
    "                                           intensity_scale_factor=2.5,\n",
    "                                           metric=\"cosine\",\n",
    "                                           plot_histogram=True,\n",
    "                                           bins=50)\n",
    "\n",
    "knn_train = tp.plot_utils.plot_knn_examples(output_head_train.embeddings,\n",
    "                                            loader_train,\n",
    "                                            n_neighbors=5,\n",
    "                                            examples=random_samples,\n",
    "                                            cmap=plt.cm.viridis,\n",
    "                                            intensity_scale_factor=2.5,\n",
    "                                            metric=\"cosine\",\n",
    "                                            plot_histogram=True,\n",
    "                                            bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(maps_bbone_train_v2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(maps_bbone_test_v1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(maps_bbone_test_v2[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-mouth",
   "metadata": {},
   "source": [
    "## Maps of the projection head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"computing the maps\")\n",
    "\n",
    "maps_head_test_intra = plot_all_maps(embed_output=output_head_test,\n",
    "                                     cmap=plt.cm.inferno,\n",
    "                                     figsize=(5,5),\n",
    "                                     remove_intra_tissue_connectivity=False,\n",
    "                                     title_suffix=\"test intra\")\n",
    "\n",
    "maps_head_train_intra = plot_all_maps(embed_output=output_head_train,\n",
    "                                      cmap=plt.cm.inferno,\n",
    "                                      figsize=(5,5),\n",
    "                                      remove_intra_tissue_connectivity=False,\n",
    "                                      title_suffix=\"train intra\")\n",
    "\n",
    "maps_head_test_NO_intra = plot_all_maps(embed_output=output_head_test,\n",
    "                                        cmap=plt.cm.inferno,\n",
    "                                        figsize=(5,5),\n",
    "                                        remove_intra_tissue_connectivity=True,\n",
    "                                        title_suffix=\"test NO intra\")\n",
    "\n",
    "maps_head_train_NO_intra = plot_all_maps(embed_output=output_head_train,\n",
    "                                         cmap=plt.cm.inferno,\n",
    "                                         figsize=(5,5),\n",
    "                                         remove_intra_tissue_connectivity=True,\n",
    "                                         title_suffix=\"train NO intra\")\n",
    "\n",
    "for tmp in maps_head_test_intra:\n",
    "    neptune_logger.log_image(\"maps_head_test_intra\", tmp)\n",
    "    \n",
    "for tmp in maps_head_train_intra:\n",
    "    neptune_logger.log_image(\"maps_head_train_intra\", tmp)\n",
    "    \n",
    "for tmp in maps_head_test_NO_intra:\n",
    "    neptune_logger.log_image(\"maps_head_test_NO_intra\", tmp)\n",
    "\n",
    "for tmp in maps_head_train_NO_intra:\n",
    "    neptune_logger.log_image(\"maps_head_train_NO_intra\", tmp)\n",
    "\n",
    "print(\"maps logged into neptune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-breach",
   "metadata": {},
   "source": [
    "# Train a classifier to find the tissue labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_head_test.embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = output_head_test.embeddings.cpu().numpy() \n",
    "y = numpy.array(output_head_test.fnames)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
    "    \n",
    "classifier = MLPClassifier(random_state=1, hidden_layer_sizes=2, max_iter=30000, tol=1E-8, n_iter_no_change=100)\n",
    "classifier.fit(X_train, y_train) \n",
    "y_predicted = classifier.predict(X_test)\n",
    "accuracy = numpy.mean((y_test == y_predicted).astype(float))\n",
    "print(\"accuracy ->\", accuracy)\n",
    "plt.plot(classifier.loss_curve_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = output_head_train.embeddings.cpu().numpy() \n",
    "y = numpy.array(output_head_test.fnames)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=1)\n",
    "    \n",
    "classifier = MLPClassifier(random_state=1, hidden_layer_sizes=2, max_iter=30000, tol=1E-6, n_iter_no_change=100)\n",
    "classifier.fit(X_train, y_train) \n",
    "y_predicted = classifier.predict(X_test)\n",
    "accuracy = numpy.mean((y_test == y_predicted).astype(float))\n",
    "print(\"accuracy ->\", accuracy)\n",
    "plt.plot(classifier.loss_curve_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-insulin",
   "metadata": {},
   "source": [
    "# STOP HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-ministry",
   "metadata": {},
   "source": [
    "### Save the trained model locally and in Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs(model_folder, exist_ok=True)\n",
    "#torch.save(model.state_dict(), os.path.join(model_folder, \"simclr_model.pt\"))\n",
    "#neptune_logger.log_artifact(os.path.join(model_folder, \"simclr_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-spice",
   "metadata": {},
   "source": [
    "## TensorFlow Projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings=embeddings_bbone_test\n",
    "#tissue_list = tissue_list_test\n",
    "#condition_list=condition_list_test\n",
    "#morant_value=morant_value_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-priest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy\n",
    "#\n",
    "#tp.evaluation_utils.create_projector(\n",
    "#    testloader, embeddings, {\"tissue\": tissue_list,\n",
    "#                             \"condition\": condition_list, \n",
    "#                             \"moran\": morant_value.tolist()}, apply_compress=False\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir ./projector --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-surge",
   "metadata": {},
   "source": [
    "## Create a dataset to train a Linear Classifier by subsampling and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_df = [df_wt1, df_wt2, df_wt3, df_dis1, df_dis2, df_dis3]\n",
    "#all_labels_sparse_images = [0, 0, 0, 1, 1, 1]\n",
    "#all_names_sparse_images = [\"wt1\", \"wt2\", \"wt3\", \"dis1\", \"dis2\", \"dis3\"]\n",
    "#all_sparse_images = [tp.data_utils.SparseImage.from_panda(\n",
    "#      df, \"x\", \"y\", category=\"cell_type\", pixel_size=config[\"simulation\"][\"PIXEL_SIZE\"], padding=10) \n",
    "#                   for df in all_df]\n",
    "\n",
    "#config[\"simulation\"][\"N_CROPS_TEST_FOR_TISSUE\"] = 24\n",
    "#\n",
    "#index = [0,1,3,4]\n",
    "#suffix = \"_\".join([str(tmp) for tmp in index])\n",
    "#name = \"classifier_data_\"+suffix+\".pt\"\n",
    "#saved_file = os.path.join(model_folder, name)\n",
    "#print(\"working on ->\", saved_file)\n",
    "#\n",
    "#dataloader = define_testloader([all_sparse_images[i] for i in index],\n",
    "#                               [all_labels_sparse_images[i] for i in index],\n",
    "#                               [all_names_sparse_images[i] for i in index],\n",
    "#                               config,\n",
    "#                               use_test_transform=True)\n",
    "#\n",
    "#output  = encoder.embed_by_backbone(dataloader, \n",
    "#                                    sparse_tensor_callback = SpatialAutocorrelation(modality='moran',\n",
    "#                                                                                    n_neighbours=6,\n",
    "#                                                                                    radius=None,\n",
    "#                                                                                    neigh_correct=False))\n",
    "#\n",
    "#torch.save(output, saved_file)\n",
    "#print(\"saved the file ->\", saved_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.load(\"../trained_model/classifier_data_0_1_3_4.pt\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
